<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Read Rust - Performance</title><link>https://readrust.net/</link><description>Performance posts on Read Rust</description><item><title>Tutorial: Profiling Rust applications in Docker with perf</title><link>https://gendignoux.com/blog/2019/11/09/profiling-rust-docker-perf.html</link><description><![CDATA[With Moore’s law coming to an end, optimizing code to avoid performance pitfalls is becoming more and more useful. To this end, programming languages like Rust are designed to produce fast and memory-efficient programs out-of-the-box. When that is not sufficient, profilers like perf are useful to measure where the code is slow and therefore which algorithms and data structures should be optimized.]]></description><guid isPermaLink="false">2de8fbcd-1a56-4eea-8f6d-54cb843418e2</guid><pubDate>Sat,  9 Nov 2019 00:00:00 +0000</pubDate><dc:creator>Guillaume Endignoux</dc:creator></item><item><title>Battle of the Serverless — Part 2: AWS Lambda Cold Start Times</title><link>https://medium.com/@shouldroforion/battle-of-the-serverless-part-2-aws-lambda-cold-start-times-1d770ef3a7dc</link><description><![CDATA[This experiment continues the work done in our pretend suite of microservices exposed via API Gateway to form an API with a code name of Slipspace in a mock company called STG. Slipspace drives are how the ships in the Halo universe travel so quickly to different sectors of the galaxy through something called Slipstream Space, so thought it was cool for a name requiring awesome warp API speeds.]]></description><guid isPermaLink="false">48cf21ce-5244-4667-82c3-9c54bb469bbf</guid><pubDate>Thu,  7 Nov 2019 03:15:40 +0000</pubDate><dc:creator>Mark Fowler</dc:creator></item><item><title>Comparing parallel Rust and C++</title><link>https://parallel-rust-cpp.github.io/</link><description><![CDATA[In this tutorial, we will implement a Rust program that attempts to utilize 100% of the theoretical capacity of three relatively modern, mid-range CPUs. We'll use an existing, highly efficient C++ implementation as a reference point to compare how our Rust program is doing. We start with a simple baseline solution of 3 nested for-loops, and keep improving on the baseline solution incrementally, implementing 8 versions in total, until the program is going so fast it can hardly go faster. We'll approach the problem from the point of view of a C++ programmer who already knows how the reference implementation solves the problem, but is interested in an approach using the Rust language.]]></description><guid isPermaLink="false">9e49b1b3-2b2a-46f0-951e-bda659adca6e</guid><pubDate>Thu,  7 Nov 2019 00:00:00 +0000</pubDate><dc:creator>Matias Lindgren</dc:creator></item><item><title>Always Bump Downwards</title><link>https://fitzgeraldnick.com/2019/11/01/always-bump-downwards.html</link><description><![CDATA[When writing a bump allocator, always bump downwards. That is, allocate from high addresses, down towards lower addresses by decrementing the bump pointer. Although it is perhaps less natural to think about, it is more efficient than incrementing the bump pointer and allocating from lower addresses up to higher ones.]]></description><guid isPermaLink="false">a251550c-42ad-42d6-8780-eae69e3bc85f</guid><pubDate>Fri,  1 Nov 2019 00:00:00 -0700</pubDate><dc:creator>Nick Fitzgerald</dc:creator></item><item><title>“Beating C” with 120 Lines of Rust: wc</title><link>https://medium.com/@martinmroz/beating-c-with-120-lines-of-rust-wc-a0db679fe920</link><description><![CDATA[It’s something of a meme lately to see whether your programming language of choice can take on the venerable wc, and what that might look like. The format seems to be: first do it simply, then idiomatically, and finally much faster. Of course, we’re not really “beating C” but rather “tackling a fun interview question in our favorite programming language.” My go-to these days is Rust, and since I’ve fielded the question of whether Rust is “my Haskell,” this all was too much to pass up. Let’s get started.]]></description><guid isPermaLink="false">7eda1c5a-b463-4d8b-a024-68fe21c735d0</guid><pubDate>Wed, 30 Oct 2019 07:07:06 +0000</pubDate><dc:creator>Martin Mroz</dc:creator></item><item><title>FastSpark: A New Fast Native Implementation of Spark from Scratch</title><link>https://medium.com/@rajasekar3eg/fastspark-a-new-fast-native-implementation-of-spark-from-scratch-368373a29a5c</link><description><![CDATA[I got a project idea to test the feasibility of implementing Spark in a native language and if feasible, explore how efficient it can be in terms of performance and resource management. I know that Spark is heavily optimized over the years. I didn’t hope for any drastic difference in performance and if some difference is there, it most likely will be in RAM usage. Also, I want it to very general-purpose just like Spark. I decided to use Rust for the implementation.]]></description><guid isPermaLink="false">ca9e7e60-b2dd-4481-9d5b-82a5936d8d9c</guid><pubDate>Wed, 23 Oct 2019 02:26:17 +0000</pubDate><dc:creator>Raja Sekar</dc:creator></item><item><title>Where rustc spends its time</title><link>https://wiki.alopex.li/WhereRustcSpendsItsTime</link><description><![CDATA[So a couple weeks ago I was a little stung by the quote from This Week In Rust: “Rust compilation is so slow that I can fix the bugs while it still compiles the crates”. On the one hand, I have unfond memories of waiting for a Typescript project to compile, pack (aka link), minify (aka optimize), and so on, over and over, on every change. At least if it had been Rust I’d have been able to fix the bugs while it was doing this. On the other hand, it’s also mostly true: compiling Rust is heckin’ slow. So I’ve decided to dust off a backburner project for a while, and figure out just where rustc spends most of its time.]]></description><guid isPermaLink="false">5babdb55-5061-4e26-b05e-a7c407d03c80</guid><pubDate>Wed, 23 Oct 2019 00:00:00 +1100</pubDate><dc:creator>Simon Heath</dc:creator></item><item><title>Rust Big Data Benchmarks</title><link>https://andygrove.io/rust_bigdata_benchmarks/</link><description><![CDATA[I have been running benchmarks of aggregate queries against the NYC taxi data set, using Apache Spark (JVM-based) as the baseline, since it is currently a popular tool for distributed compute, and a tool I am familiar with.]]></description><guid isPermaLink="false">59f60adc-9c66-4f8b-9a3a-6ffdcedb14c6</guid><pubDate>Sun, 20 Oct 2019 00:00:00 +0000</pubDate><dc:creator>Andy Grove</dc:creator></item><item><title>Rust and C++ on Floating-point Intensive Code</title><link>https://www.reidatcheson.com/hpc/architecture/performance/rust/c++/2019/10/19/measure-cache.html</link><description><![CDATA[Since I do a lot of heavy numeric computation in C++ it was tempting for me to see how Rust compares in a shootout. I chose a floating point benchmark to implement in both languages in order to see the performance difference. I give commentary on why the performance is that way, and some potential fixes Rust could implement to close the gap.]]></description><guid isPermaLink="false">ce96a1d5-014d-41ed-b997-45c0449aed59</guid><pubDate>Sat, 19 Oct 2019 23:59:59 +0000</pubDate><dc:creator>Reid Atcheson</dc:creator></item><item><title>Making the Tokio scheduler 10x faster</title><link>https://tokio.rs/blog/2019-10-scheduler/</link><description><![CDATA[We’ve been hard at work on the next major revision of Tokio, Rust’s asynchronous runtime. Today, a complete rewrite of the scheduler has been submitted as a pull request. The result is huge performance and latency improvements. Some benchmarks saw a 10x speed up! It is always unclear how much these kinds of improvements impact “full stack” use cases, so we’ve also tested how these scheduler improvements impacted use cases like Hyper and Tonic (spoiler: it’s really good).]]></description><guid isPermaLink="false">97ca4165-4f8c-4c57-872d-e5986df0c7ee</guid><pubDate>Sun, 13 Oct 2019 00:00:00 +0000</pubDate><dc:creator>Carl Lerche</dc:creator></item><item><title>How to speed up the Rust compiler some more in 2019</title><link>https://blog.mozilla.org/nnethercote/2019/10/11/how-to-speed-up-the-rust-compiler-some-more-in-2019/</link><description><![CDATA[In July I wrote about my efforts to speed up the Rust compiler in 2019. I also described how the Rust compiler has gotten faster in 2019, with compile time reductions of 20-50% on most benchmarks. Now that Q3 is finished it’s a good time to see how things have changed since then.]]></description><guid isPermaLink="false">67d3364b-64e1-4c9e-bf5c-93ed324c8f4a</guid><pubDate>Thu, 10 Oct 2019 23:01:52 +0000</pubDate><dc:creator>Nicholas Nethercote</dc:creator></item><item><title>Speeding Up Rust Builds: Code-Gen Edition</title><link>https://epage.github.io/blog/2019/10/speeding-up-rust-builds-code-gen-edition/</link><description><![CDATA[Lately, there has been talk talk about improving build times, with a focus on reducing bloat like regex breaking out logic into features that can be disabled, cargo-bloat going on a diet, new cargo features to identify slow-to-build dependencies. The area that has been impacting me lately is build.rs. I've been code-generating compile-time hash tables (phf) which has added several dependencies to my build and takes a while.]]></description><guid isPermaLink="false">20e74694-1d5e-4315-9af9-b9efe6721ecc</guid><pubDate>Thu, 10 Oct 2019 03:30:17 +0000</pubDate><dc:creator>Ed Page</dc:creator></item><item><title>Visualizing Rust compilation</title><link>https://blog.mozilla.org/nnethercote/2019/10/10/visualizing-rust-compilation/</link><description><![CDATA[Speeding up the Rust compiler isn’t the only way to make a Rust project build faster. Changing the crate structure of a project can also make a big difference. The good news here is that Eric Huss has implemented an amazing tool for visualizing Rust compilation, which can be used to identify inefficient crate structures in Rust projects.]]></description><guid isPermaLink="false">7fafbd3e-01e8-46ad-9dcf-3aca241030b7</guid><pubDate>Wed,  9 Oct 2019 23:34:50 +0000</pubDate><dc:creator>Nicholas Nethercote</dc:creator></item><item><title>Dev Time Optimization -- Part 1 (1.9x speedup, 65% less disk usage)</title><link>https://azriel.im/will/2019/10/08/dev-time-optimization-part-1-1.9x-speedup-65-less-disk-usage/</link><description><![CDATA[Summary In a 45k LOC / 102-crate workspace, moving tests from member crates into a single workspace_tests crate achieved the following improvements:

Build and test duration in release mode reduced from 23 minutes to 13 minutes . Debug artifact disk usage reduced from 20 G to 7 G (65% reduction, fresh build), or 230 G to 50 G (78% reduction, ongoing development)  Background The rate of software development is affected by many limits.]]></description><guid isPermaLink="false">9058ab44-792b-4652-a06a-db4ada3e0ff5</guid><pubDate>Tue,  8 Oct 2019 13:10:38 +1300</pubDate><dc:creator>Azriel Hoh</dc:creator></item><item><title>Binary Format Shootout</title><link>https://speice.io/2019/09/binary-format-shootout.html</link><description><![CDATA[Cap'n Proto vs. Flatbuffers vs. Simple Binary Encoding]]></description><guid isPermaLink="false">87600fb3-40d3-403e-87b7-25a3a2eacdcf</guid><pubDate>Sat, 28 Sep 2019 00:00:00 +0000</pubDate><dc:creator>Bradlee Speice</dc:creator></item><item><title>Causal Profiling Rust Code</title><link>https://llogiq.github.io/2019/09/25/coz.html</link><description><![CDATA[There’s a new hotness in performance measurements, and it’s called causal profiling. The idea behind it is that you want to measure how a speed up of a certain function would impact the runtime as a whole, which can be very counterintuitive in today’s multi-threaded world.]]></description><guid isPermaLink="false">6090f016-cbc0-48bd-b448-c2dd82f721b2</guid><pubDate>Wed, 25 Sep 2019 00:00:00 +0000</pubDate><dc:creator>Llogiq</dc:creator></item><item><title>Of bitpacking, with or without SSE3</title><link>https://fulmicoton.com/posts/bitpacking/</link><description><![CDATA[This blog post might interest three type of readers: people interested in tantivy: You’ll learn how tantivy uses SIMD instructions to decode posting lists, and what happens on platform where the relevant instruction set is not available.  rustaceans who would like to hear a good SIMD in rust story. lucene core devs (yeah it is a very select club) who might be interested in a possible (unconfirmed) optimization opportunity.]]></description><guid isPermaLink="false">d7a12b5f-ab52-4e4b-b752-7d857e9f86bf</guid><pubDate>Fri, 20 Sep 2019 00:00:00 +0000</pubDate><dc:creator>Paul Masurel</dc:creator></item><item><title>Closing the gap: cross-language LTO between Rust and C/C++</title><link>http://blog.llvm.org/2019/09/closing-gap-cross-language-lto-between.html?m=1</link><description><![CDATA[Link time optimization (LTO) is LLVM's way of implementing whole-program optimization. Cross-language LTO is a new feature in the Rust compiler that enables LLVM's link time optimization to be performed across a mixed C/C++/Rust codebase.]]></description><guid isPermaLink="false">f6562f73-5770-4c71-9aad-ef1f08727f58</guid><pubDate>Thu, 19 Sep 2019 00:00:00 +0000</pubDate><dc:creator>Michael</dc:creator></item><item><title>An unexpected performance regression</title><link>https://dev.to/sharkdp/an-unexpected-performance-regression-11ai</link><description><![CDATA[A short story on how compiler updates can cause unexpected performance regressions.]]></description><guid isPermaLink="false">c14e5bd8-c28e-46c1-99dc-d3a7536ec661</guid><pubDate>Mon, 16 Sep 2019 19:16:58 +0000</pubDate><dc:creator>David Peter</dc:creator></item><item><title>Profiling augr&apos;s performance</title><link>https://geemili.xyz/blog/profiling-augr/</link><description><![CDATA[I built a time tracking software named augr, and recently decided that it was time to make it faster.]]></description><guid isPermaLink="false">929a5316-4621-43cf-9e9b-8e3c43989242</guid><pubDate>Sat,  7 Sep 2019 00:00:00 +0000</pubDate><dc:creator>LeRoyce Pearson</dc:creator></item><item><title>Improvement to the compile time of a crate</title><link>http://antoyo.ml/compilation-time-dependencies</link><description><![CDATA[For one of my projects, I need to use LLVM so I tried this cool inkwell crate that provides a mostly safe wrapper over LLVM. To my dismay, though, compiling this crate takes… a lot of time: Debug build: 1m 05s Release build: 3m 34s. By the way, I write this article for the sole purpose of trying to fix some problems there is in the crate ecosystem and by no mean I want to incriminate the author of this crate (or any other). I’ve been guilty of doing the same mistakes, but I learned from them and want other people to learn from them as well.]]></description><guid isPermaLink="false">a8b46d95-5e16-4348-abfe-b54bc46da590</guid><pubDate>Sat,  7 Sep 2019 00:00:00 +0000</pubDate><dc:creator>Antoni Boucher</dc:creator></item><item><title>How Rust optimizes async/await II: Program analysis</title><link>https://tmandry.gitlab.io/blog/posts/optimizing-await-2/</link><description><![CDATA[In Part 1, we covered how async fns in Rust are compiled to state machines. We saw that the internal compiler implementation uses generators and the yield statement to facilitate this transformation. In this post, we'll go over some subtleties that the compiler implementation must consider when optimizing generators. We'll look at two different kinds of analysis, liveness analysis and storage conflict detection.]]></description><guid isPermaLink="false">7304ebe7-ee28-48a9-b88c-dd03e2605a10</guid><pubDate>Mon,  2 Sep 2019 00:00:00 +0000</pubDate><dc:creator>Tyler Mandry</dc:creator></item><item><title>Criterion.rs v0.3 - Custom Measurements, Profiling Hooks, Custom Test Framework, API Changes</title><link>https://bheisler.github.io/post/criterion-rs-0-3/</link><description><![CDATA[I’m pleased to announce the release of Criterion.rs v0.3, available today. Version 0.3 provides a number of new features including preliminary support for plugging in custom measurements (eg. hardware timers or POSIX CPU time), hooks to start/stop profilers, a new BenchmarkGroup struct that provides more flexibility than the older Benchmark and ParameterizedBenchmark structs, and an implementation of a #[criterion] custom-test-framework macro for those on Nightly.]]></description><guid isPermaLink="false">3a399e6a-a660-464b-90c0-9cee92eade7c</guid><pubDate>Sun, 25 Aug 2019 10:30:00 -0600</pubDate><dc:creator>Brook Heisler</dc:creator></item><item><title>Building with async/await in Rust</title><link>https://ragona.com/posts/clobber_async_await</link><description><![CDATA[As you might have heard, async/await is coming to Rust soon. This is a big deal. Rust has already has popular crates (tokio, actix) that provide asynchronous concurrency, but the async syntax coming to stable in 1.39 is much, much more approachable. My experience has been that you can produce and reason about application flow much more easily, which has made me significantly more productive when dealing with highly concurrent systems. To kick the tires of this new syntax I dug into the nightly branch, and built a high-performance TCP client called clobber. In this post I'll talk about why I think async/await in Rust is a big deal, and walk you some of the code in clobber.]]></description><guid isPermaLink="false">b577a6bb-429a-454b-8d2b-77175dba68c8</guid><pubDate>Wed, 21 Aug 2019 00:00:00 +0000</pubDate><dc:creator>Ryan Ragona</dc:creator></item><item><title>How Rust optimizes async/await: Part 1</title><link>https://tmandry.gitlab.io/blog/posts/optimizing-await-1/</link><description><![CDATA[The issue to stabilize an initial version of async/await in Rust has left final comment period. The feature looks slated to stabilize in an upcoming release, most likely 1.39. One of the blockers mentioned in the RFC is the size of the state machines emitted by async fn. I’ve spent the last few months tackling this problem, and wanted to give people a window into the process of writing these optimizations, with all the intricacies involved.]]></description><guid isPermaLink="false">c0401410-c335-43a2-a9f3-342909efdb3f</guid><pubDate>Sun, 18 Aug 2019 00:00:00 +0000</pubDate><dc:creator>Tyler Mandry</dc:creator></item><item><title>The Rust compiler is still getting faster</title><link>https://blog.mozilla.org/nnethercote/2019/07/25/the-rust-compiler-is-still-getting-faster/</link><description><![CDATA[I previously wrote about one period of improvement in Rust compiler speed. How are things going in 2019?]]></description><guid isPermaLink="false">771ffeb1-3f31-43b3-9f4b-17b114e326c2</guid><pubDate>Thu, 25 Jul 2019 03:56:24 +0000</pubDate><dc:creator>Nicholas Nethercote</dc:creator></item><item><title>C vs Rust vs Go: performance analysis</title><link>https://medium.com/@marek.michalik/c-vs-rust-vs-go-performance-analysis-945ab749056c</link><description><![CDATA[In one of my previous jobs I got a task: “For given image find popular colors in that image, so users can browse images by it’s colors”. This is where three languages comes to play. I have implemented histogram algorithm in C, Rust and Go.]]></description><guid isPermaLink="false">13003d78-0993-49fe-ae7e-0b0263a77353</guid><pubDate>Thu, 18 Jul 2019 20:54:01 +0000</pubDate><dc:creator>Marek Michalik</dc:creator></item><item><title>How to speed up the Rust compiler in 2019</title><link>https://blog.mozilla.org/nnethercote/2019/07/17/how-to-speed-up-the-rust-compiler-in-2019/</link><description><![CDATA[I have written previously about my efforts to speed up the Rust compiler in 2016 (part 1, part 2) and 2018 (part 1, part 2, NLL edition). It’s time for an update on the first half of 2019.]]></description><guid isPermaLink="false">688a7c2c-4cb7-4da6-a9d5-3cf4d5f7571a</guid><pubDate>Wed, 17 Jul 2019 02:54:57 +0000</pubDate><dc:creator>Nicholas Nethercote</dc:creator></item><item><title>CPU atomics and orderings explained</title><link>https://fy.blackhats.net.au/blog/html/2019/07/16/cpu_atomics_and_orderings_explained.html</link><description><![CDATA[Sometimes the question comes up about how CPU memory orderings work, and what they do. I hope this post explains it in a really accessible way.]]></description><guid isPermaLink="false">fb9caad4-9aa0-4996-92d6-93fd75509dcf</guid><pubDate>Tue, 16 Jul 2019 00:00:00 +0000</pubDate><dc:creator>Firstyear</dc:creator></item><item><title>cedarwood: Efficiently-Updatable Double Array Trie in Rust</title><link>https://blog.paulme.ng/posts/2019-07-14-cedarwood:-efficiently-updatable-double-array-trie-in-rust.html</link><description><![CDATA[Cedarwood is an effort to speed up jieba-rs, an efficient implementation of trie is needed in order to satisfying the following needs.]]></description><guid isPermaLink="false">44e12e0f-4572-479e-a63c-c4539b2c9ae8</guid><pubDate>Sun, 14 Jul 2019 00:00:00 +0000</pubDate><dc:creator>Paul Meng</dc:creator></item><item><title>Introducing glam and mathbench</title><link>http://bitshifter.github.io/2019/07/10/introducing-glam-and-mathbench/</link><description><![CDATA[glam is a simple and fast Rust linear algebra library for games and graphics. mathbench is a set of unit tests and benchmarks comparing the performance of glam with the popular Rust linear algebra libraries cgmath and nalgebra. The following is a table of benchmarks produced by mathbench comparing glam performance to cgmath and nalgebra on f32 data.]]></description><guid isPermaLink="false">5f582def-af9d-44cb-9a18-648bff14bd67</guid><pubDate>Wed, 10 Jul 2019 00:00:00 +0000</pubDate><dc:creator>Cameron Hart</dc:creator></item><item><title>The Computer Language Benachmarks Game: Rust ranks #1 for n-body</title><link>https://frehberg.com/2019/07/the-computer-language-benachmarks-game-rust-ranks-1-for-n-body/</link><description><![CDATA[The Computer Language Benchmarks Game is a free software project for comparing how a given subset of simple algorithms can be implemented in various popular programming languages. I converted the fastest (dating early 2019) n-body C-implementation (#4) to Rust (#7) in a one-to-one fashion, gaining a performance encreasement by factor 1.6 to my own surprise.]]></description><guid isPermaLink="false">07e226de-482b-4f51-af76-adb65b4a35dc</guid><pubDate>Tue,  9 Jul 2019 16:53:57 +0000</pubDate><dc:creator>Frank Rehberger</dc:creator></item><item><title>TLS performance: rustls versus OpenSSL</title><link>https://jbp.io/2019/07/01/rustls-vs-openssl-performance.html</link><description><![CDATA[There are quite a few dimensions to how performance can vary between TLS libraries such as handshake performance and bulk performance. This series of blog posts measures and compares the performance of rustls (a TLS library in rust) and OpenSSL.]]></description><guid isPermaLink="false">ead392a4-baff-4c08-9820-8fd74ef14200</guid><pubDate>Mon,  1 Jul 2019 00:00:00 +0000</pubDate><dc:creator>Joseph Birr-Pixton</dc:creator></item><item><title>One Program Written in Python, Go, and Rust</title><link>http://www.nicolas-hahn.com/python/go/rust/programming/2019/07/01/program-in-python-go-rust/</link><description><![CDATA[This is a subjective, primarily developer-ergonomics-based comparison of the three languages from the perspective of a Python developer, but you can skip the prose and go to the code samples, the performance comparison if you want some hard numbers, the takeaway for the tl;dr, or the Python, Go, and Rust diffimg implementations.]]></description><guid isPermaLink="false">a7b2c09d-06ca-4ae2-89c2-70bf78282396</guid><pubDate>Mon,  1 Jul 2019 00:00:00 +0000</pubDate><dc:creator>Nicolas Hahn</dc:creator></item><item><title>Optimizing jieba-rs to be 33% faster than cppjieba</title><link>https://blog.paulme.ng/posts/2019-06-30-optimizing-jieba-rs-to-be-33percents-faster-than-cppjieba.html</link><description><![CDATA[This blog post is mainly to share my experience on taking an emerging programming language’s ecosystem seriously and evaluating it by working on a serious project, and see how far we can go in terms of performance and development experience. The project I chose as mentioned in the title is jieba-rs, the rust implementation of a popular Chinese word segmentation library: Jieba.]]></description><guid isPermaLink="false">9618e311-ec84-431c-97ab-c9f010c3c65c</guid><pubDate>Sun, 30 Jun 2019 00:00:00 +0000</pubDate><dc:creator>Paul Meng</dc:creator></item><item><title>Writing a small ray tracer in Rust and Zig</title><link>https://nelari.us/post/raytracer_with_rust_and_zig/</link><description><![CDATA[The two languages that I spent most of my time daydreaming about writing code in are Rust and Zig. Would the lack of features in Zig make me more or less productive than with Rust’s feature overload? Which language is more enjoyable to use for writing a small, self-contained computer graphics project?  To find out, I decided to implement the same simple project in both languages: a small ray tracer, following the book Ray Tracing in One Weekend.]]></description><guid isPermaLink="false">6e2a2811-942b-4e62-b173-6d2b0d3e0acd</guid><pubDate>Thu, 27 Jun 2019 20:00:00 +0200</pubDate><dc:creator>Johann Muszynski</dc:creator></item><item><title>RESS (Rusty EcmaScript Scanner) 0.7.0 bring large performance improvements</title><link>https://wiredforge.com/blog/ress-7/</link><description><![CDATA[A blog about learning computer science concepts with practical projects]]></description><guid isPermaLink="false">876033ef-0f64-4108-89c6-8ce8d6536ac7</guid><pubDate>Wed, 26 Jun 2019 00:00:00 +0000</pubDate><dc:creator>Robert Masen</dc:creator></item><item><title>The smooth resize test</title><link>https://raphlinus.github.io/rust/gui/2019/06/21/smooth-resize-test.html</link><description><![CDATA[Today is an exciting point in the evolution of native GUI in Rust. There is much exploration, and a number of promising projects, but I also think we don’t yet know the recipe to make GUI truly great. As I develop my own vision in this space, druid, I hope more that the efforts will learn from each other and that an excellent synthesis will emerge, more so than simply hoping that druid will win.

In my work, I have come across a problem that is as seemingly simple, yet as difficult to get right, as making decent tea: handling smooth window resizing. Very few GUI toolkits get it perfect, with some failing spectacularly. This is true across platforms, though Windows poses special challenges. It’s also pretty easy to test (as opposed to requiring sophisticated latency measurements, which I also plan to develop). I suggest it become one of the basic tests to evaluate a GUI toolkit.]]></description><guid isPermaLink="false">1898399d-d60a-46fc-a3e0-cb8a73fb546e</guid><pubDate>Fri, 21 Jun 2019 19:50:42 +0000</pubDate><dc:creator>Raph Levien</dc:creator></item><item><title>On Memoization in Rust</title><link>https://medium.com/swlh/on-memoization-291fd1dd924</link><description><![CDATA[A detailed walk through how to memoize function calls in Rust.]]></description><guid isPermaLink="false">b26fcca3-2a79-4340-88fd-80c6f70c95d1</guid><pubDate>Mon, 17 Jun 2019 12:30:14 +0000</pubDate><dc:creator>Andrew Pritchard</dc:creator></item><item><title>Speed Up Your JavaScript With Rust</title><link>https://medium.com/paloit/speed-up-your-javascript-with-rust-7661922562fa</link><description><![CDATA[For a recent personal project, I had only needed a fairly simple node.js server to do exponential and costly computing tasks. To be honest, I could have switched the entire tech stack, but I estimated that the development time of such a choice wasn’t worth it… Still, I had some functions taking ages to compute. So I had a look around, and decided to let that task be handled by a more appropriate language, in this case Rust.]]></description><guid isPermaLink="false">e3be782c-4057-4e64-9c1a-0ff81761d5d7</guid><pubDate>Thu,  6 Jun 2019 01:54:49 +0000</pubDate><dc:creator>Johan Paasche</dc:creator></item><item><title>Speeding up Ruby MRI with Rust</title><link>https://medium.com/@flixdescteaux/speeding-up-ruby-mri-with-rust-a7c914d2f9d0</link><description><![CDATA[Let me start by saying I really like Ruby. I tend to agree with the statement saying Ruby is optimized for developer happiness. However, nothing comes for free. Programming ecstasy is a double-edged sword and writing slow Ruby is as easy as it is pleasant.]]></description><guid isPermaLink="false">63fcd49a-f8d8-46f4-92dd-58cb8d77082c</guid><pubDate>Tue, 21 May 2019 13:58:31 +0000</pubDate><dc:creator>Félix Descôteaux</dc:creator></item><item><title>Momo · Get Back Some Compile Time From Monomorphization</title><link>https://llogiq.github.io/2019/05/18/momo.html</link><description><![CDATA[Monomorphization has one problem (apart from being a ridiculous word that I’ll probably spell wrong every time): It generates rather a lot of code, bloating binary size and potentially pessimizing execution cache usage. Often, generics aren’t really needed for speed, but for ergonomics: Library code might want to present an easy-to-use generic interface that will automate some conversions. However, this often means that almost each user gets their own version of the code, leading to the aforementioned bloat (case in point: Earlier clap versions were notorious for adding hundreds of kilobytes to the binary size – for a simple command line parser).]]></description><guid isPermaLink="false">568fdc54-427f-47be-8849-dd040bafe8dd</guid><pubDate>Sat, 18 May 2019 00:00:00 +0000</pubDate><dc:creator>Llogiq</dc:creator></item><item><title>Using Rust to Scale Elixir for 11 Million Concurrent Discord Users</title><link>https://blog.discordapp.com/using-rust-to-scale-elixir-for-11-million-concurrent-users-c6f19fc029d3</link><description><![CDATA[Over the last year, the Backend Infrastructure team at Discord was hard at work improving the scalability and performance of our core real-time communications infrastructure. One big project we undertook was changing how we update the Member List.]]></description><guid isPermaLink="false">2d643e8c-a753-49c4-b18a-3323bb783259</guid><pubDate>Fri, 17 May 2019 15:42:16 +0000</pubDate><dc:creator>Matt Nowack</dc:creator></item><item><title>Racing sed with Rust</title><link>https://www.lambdafunctions.com/articles/racing-sed-with-rust</link><description><![CDATA[As part of a project I’m working on, I sometimes find myself having to deal with quite large X12 files. What I’d really like is a small, self-contained tool that I can pass an X12 file to and rely on it to Do The Right Thing™ without any unnecessary incantations. Since I’m dealing with large source files it would also be nice if it was at least as fast as standard tools like sed. Sounds like a job for…]]></description><guid isPermaLink="false">7d2c4d88-ce3c-4794-8093-80d1dd15395c</guid><pubDate>Tue,  7 May 2019 00:00:00 +0000</pubDate><dc:creator>Mike Clarke</dc:creator></item><item><title>Rust concurrency patterns: Communicate by sharing your sender(re-visited)</title><link>https://medium.com/@polyglot_factotum/rust-concurrency-patterns-communicate-by-sharing-your-sender-re-visited-9d42e6dfecfa</link><description><![CDATA[Since I last wrote about this topic, just only about a year ago select as used in the standard-library channel, has been deprecated. So it’s a good time to re-visit some of the concepts in that article, this time in the context of using crossbeam channels, and instead of using a made-up example, let’s dig into some real “production” code, as found in Servo. Let’s continue our exploration of Rust concurrency…]]></description><guid isPermaLink="false">5cea79c2-18d1-470d-b0a4-8647f0e2e5ca</guid><pubDate>Sun,  5 May 2019 06:18:08 +0000</pubDate><dc:creator>Gregory Terzian</dc:creator></item><item><title>Rust parallelism for non-C/C++ developers</title><link>https://medium.com/nearprotocol/rust-parallelism-for-non-c-c-developers-ec23f48b7e56</link><description><![CDATA[Majority of the people coming to Rust have C/C++ background which allows them to easily transition into Rust parallelism since it is so similar. However, for many people coming from other languages, it is a challenge. In this post, we will walk through the standard Rust parallelism tools as well as the motivation behind them. This will require a hardware deep dive at the beginning, followed by an explanation of the low-level tools, like atomics, and ending with an explanation of high-level tools like Mutex. Finally, we will explain how Rust guarantees safety in multi-threaded applications.]]></description><guid isPermaLink="false">329ef1a1-d5e3-49df-87e6-b4f40038d4d8</guid><pubDate>Thu,  2 May 2019 21:32:53 +0000</pubDate><dc:creator>Maksym Zavershynskyi</dc:creator></item><item><title>DataFusion 0.13.0 Benchmarks</title><link>https://andygrove.io/2019/04/datafusion-0.13.0-benchmarks/</link><description><![CDATA[Over the past couple weeks I’ve been working on a couple different efforts around parallel query execution with DataFusion: 1. Benchmarking parallel query execution by manually creating one execution context per parquet partition and running on a thread, just to get an idea of expected performance, and comparing results to Apache Spark (running in local mode). 2. Creating a PoC of actual parallel query execution in the Arrow/DataFusion repository. This post is mostly about the first effort.]]></description><guid isPermaLink="false">e25d035f-9121-4cd6-b0ac-f98db53fb3e5</guid><pubDate>Sun, 28 Apr 2019 00:00:00 +0000</pubDate><dc:creator>Andy Grove</dc:creator></item><item><title>Arenas vs. Indices</title><link>https://llogiq.github.io/2019/04/06/arena.html</link><description><![CDATA[When optimizing code, one thing I’m always looking for is memory layout and access patterns. One such pattern is an arena: Reserve some sufficiently large space to put your objects in, then allocate by incrementing a pointer. If your objects are of a uniform type, you can basically simplify this to a Vec of that type.]]></description><guid isPermaLink="false">5af2fc94-f73d-498a-99b8-15400fad948e</guid><pubDate>Sat,  6 Apr 2019 00:00:00 +0000</pubDate><dc:creator>Llogiq</dc:creator></item><item><title>RaptorQ (RFC6330) and performance optimization in Rust</title><link>https://www.cberner.com/2019/03/30/raptorq-rfc6330-rust-optimization/</link><description><![CDATA[I recently decided to learn more about Rust, and wrote a high performance RaptorQ (RFC6330) library. RaptorQ is a fountain code, and the core of the algorithm is a lot of matrix math over GF(256) – which translates into lots of XORs and reads from lookup tables. After getting the initial implementation working, I set about optimizing it. Below is a journal of the steps I took to profile and optimize the implementation.]]></description><guid isPermaLink="false">eb668d47-71e6-4379-a39c-3abeadd136b3</guid><pubDate>Sat, 30 Mar 2019 02:11:47 +0000</pubDate><dc:creator>Christopher Berner</dc:creator></item><item><title>Helix: Improve the Performance of Rails with Rust</title><link>https://headway.io/blog/helix-improve-the-performance-of-rails-with-rust/</link><description><![CDATA[With Rust, you can do low-level number-crunching and bit-by-bit processing, while enjoying memory safety and concurrency features. With Helix, you can use your Rust code inside of a Rails project.]]></description><guid isPermaLink="false">1f1562d2-b784-4bda-84d4-a0dc68178793</guid><pubDate>Fri, 22 Mar 2019 00:00:00 +0000</pubDate><dc:creator>Raphael Spencer</dc:creator></item><item><title>Why Hashbrown Does A Double-Lookup</title><link>https://gankro.github.io/blah/hashbrown-insert/</link><description><![CDATA[I recently finished a detailed review of hashbrown, which will likely become the new implementation for rust's std::collections::HashMap. One of the most surprising things I found was in the implementation of insert. It was doing something that was so offensive to people who care about collection performance that we had designed an entire API to help people avoid it: it did two lookups in the map. However, after some more discussion and review, I concluded that this implementation was reasonable. This post will try to cover why that is.]]></description><guid isPermaLink="false">38a15a60-b3e5-49c5-9036-3af3d6bc43ff</guid><pubDate>Wed, 20 Mar 2019 00:00:00 +0000</pubDate><dc:creator>Alexis Beingessner</dc:creator></item><item><title>Rust for Machine Learning: SIMD, BLAS, and Lapack</title><link>https://www.erikpartridge.com/2019-03/rust-ml-simd-blas-lapack</link><description><![CDATA[I love Rust. But, as a data scientist, it’s still hard to use Rust on a daily basis. 90% of my programming these days is in Python.

My interest in Rust-based machine learning sparked several months ago. But, the key limitation I found was the lack of an ergonomic linear algebra library. There’s nalgebra and ndarray and a few others. Yet, I found none of them at the time ergonomic to work with, nor fast in comparison to writing the lower-level SIMD, BLAS, and Lapack code (I have picked up ndarray more in recent weeks and months).

While inconvenient, a few months later, I’m glad I had to take things a step further. Rust is great for writing performant code. The resources for writing quite low-level mathematics operations in Rust are quite good. Using blas-src and lapack-src, as well as Rust’s built in SIMD functions, we can write fast and surprisingly portable Rust code. You can even run Rust on the GPU using, at least, the same underlying code.]]></description><guid isPermaLink="false">d937feae-59ca-4312-bd40-dc77317e441a</guid><pubDate>Sat, 16 Mar 2019 00:00:00 +0000</pubDate><dc:creator>Erik Partridge</dc:creator></item><item><title>From 48s to 5s - optimizing a 350 line pathtracer in Rust</title><link>https://medium.com/@cfsamson/from-48s-to-5s-optimizing-a-350-line-pathtracer-in-rust-191ab4a1a412</link><description><![CDATA[In this post I’ll talk about how to port a short raytracer written in C#/C++ codebase to Rust, then applying some simple optimizations by leveraging some features in Rust.]]></description><guid isPermaLink="false">ca80053b-f874-4631-9f13-c8ffaf6dabd6</guid><pubDate>Mon,  4 Mar 2019 23:03:25 +0000</pubDate><dc:creator>Carl Fredrik Samson</dc:creator></item><item><title>Fearless concurrency: how Clojure, Rust, Pony, Erlang and Dart let you achieve that</title><link>https://sites.google.com/a/athaydes.com/renato-athaydes/posts/fearlessconcurrencyhowclojurerustponyerlanganddartletyouachievethat</link><description><![CDATA[several models that make it easier to reason about concurrent programs have been envisioned over time. In this article, we'll have a quick look at a few of them, from new to not-so-new languages. I don't intend to give an extensive analysis of each solution, or make a formal comparison between them. My intention is to simply explain the basics of each solution and how they can be used in practice (with code samples that show off what the result of using the models might look like), so that other developers may have an easier time understanding them and deciding which solution, or language, might be better applicable to their particular problems.]]></description><guid isPermaLink="false">938a7c8a-5c3d-4adb-82e0-caf416396075</guid><pubDate>Sun, 24 Feb 2019 09:13:00 +0000</pubDate><dc:creator>Renato Athaydes</dc:creator></item><item><title>Rewriting stackcollapse-xdebug in Rust</title><link>https://daniellockyer.com/rewriting-stackcollapse-xdebug/</link><description><![CDATA[A week or so ago, I saw the inferno project mentioned on the Rust subreddit. It was a rewrite of the great FlameGraph library into Rust. All of the work was being livestreamed by Jon Gjengset. I ended up watching some of the livestreams and had the idea of porting the stackcollapse-xdebug.php file to Rust, potentially so it could be included in the project in the future.]]></description><guid isPermaLink="false">9494ac41-95c6-43e8-86ef-660443465886</guid><pubDate>Fri,  8 Feb 2019 00:00:00 +0000</pubDate><dc:creator>Daniel Lockyer</dc:creator></item><item><title>Solving Advent of Code in Under a Second</title><link>https://www.forrestthewoods.com/blog/solving-advent-of-code-in-under-a-second/</link><description><![CDATA[Algorithms and optimization to solve all Advent of Code 2018 puzzles in under one total second.]]></description><guid isPermaLink="false">b7475efb-632c-4875-ab6b-d283e220a434</guid><pubDate>Sun,  3 Feb 2019 00:00:00 +0000</pubDate><dc:creator>Forrest Smith</dc:creator></item><item><title>Lock-free Rust: Crossbeam in 2019</title><link>https://stjepang.github.io/2019/01/29/lock-free-rust-crossbeam-in-2019.html</link><description><![CDATA[This is a follow-up post to Lock-freedom without garbage collection from 2015, which introduced Crossbeam, a Rust library that implements efficient lock-free data structures without relying on a tracing garbage collector.]]></description><guid isPermaLink="false">7d5bbb25-d599-43d5-82cb-1a4dcdf47286</guid><pubDate>Tue, 29 Jan 2019 00:00:00 +0000</pubDate><dc:creator>Stjepan Glavina</dc:creator></item><item><title>Rust benchmarking with Criterion on Travis CI</title><link>https://medium.com/@yamafaktory/rust-benchmarking-with-criterion-on-travis-ci-%EF%B8%8F-8b54d321e05</link><description><![CDATA[Criterion allows you to benchmark against Rust stable, but it’s also providing a set of awesome features: Statistics: Statistical analysis detects if, and by how much, performance has changed since the last benchmark run. Charts: Uses gnuplot to generate detailed graphs of benchmark results. Stable-compatible: Benchmark your code without installing nightly Rust.]]></description><guid isPermaLink="false">04aedaf8-e7fe-4b54-8b6c-9fb77e31f739</guid><pubDate>Sun, 27 Jan 2019 22:14:12 +0000</pubDate><dc:creator>Davy Duperron</dc:creator></item><item><title>Guidelines on Benchmarking and Rust</title><link>https://nbsoftsolutions.com/blog/guidelines-on-benchmarking-and-rust</link><description><![CDATA[This post covers: Benchmark reports for contributors, Benchmark reports for users, Profiling with valgrind / kcachegrind, Reproducible benchmarks and graphics, and Tips for benchmark behavior and benchmarking other languages.]]></description><guid isPermaLink="false">b6fc1bbd-26d3-4334-86cc-3485ec7f4dc4</guid><pubDate>Sun, 27 Jan 2019 00:00:00 +0000</pubDate><dc:creator>Nick Babcock</dc:creator></item><item><title>Embedded Rust Experiments - Is my STM32 MCU running fast?</title><link>https://nercury.github.io/rust/embedded/experiments/2019/01/27/rust-embedded-02-measuring-the-clock.html</link><description><![CDATA[So, I have this STM32VLDISCOVERY dev board. It has the STM32F100RBT6B MCU, capable of running at 24MHz. On the board, there is a 8MHz crystal. Naturally, when you are new to microcontrollers (like me), you may have a few questions: When we upload a program on this development board, at what speed it is actually running? Is it using this external crystal? Why is this crystal 8MHz if the MCU is capable of 24MHz? If our program is not running at the maximum speed, how do we make it run at the maximum speed?]]></description><guid isPermaLink="false">c032fc9f-79b2-454f-914f-a32fda90092d</guid><pubDate>Sun, 27 Jan 2019 00:00:00 +0000</pubDate><dc:creator>Nerijus Arlauskas</dc:creator></item><item><title>Performance of Rust&apos;s match vs. lookup tables</title><link>https://kevinlynagh.com/notes/match-vs-lookup/</link><description><![CDATA[I’ve been getting into bioinformatics algorithms lately and ran across an interesting pull request that improved performance by changing a Rust match expression to a lookup. This felt quite surprising to me since, well, the match is so simple — why isn’t the compiler already generating optimal code for it?]]></description><guid isPermaLink="false">2d51794e-d4d6-44a1-a2ab-f04940842e84</guid><pubDate>Tue, 22 Jan 2019 00:00:00 +0000</pubDate><dc:creator>Kevin Lynagh</dc:creator></item><item><title>Comparing Pythagorean triples in C++, D, and Rust</title><link>https://atilanevesoncode.wordpress.com/2018/12/31/comparing-pythagorean-triples-in-c-d-and-rust/</link><description><![CDATA[You may have recently encountered and/or read this blog post criticising a possible C++20 implementation of Pythagorean triples using ranges. In it the author benchmarks different implemetations of the problem, comparing readability, compile times, run times and binary sizes. My main language these days is D, and given that D also has ranges (and right now, as opposed to a future version of the language), I almost immediately reached for my keyboard. By that time there were already some D and Rust versions floating about as a result of the reddit thread, so fortunately for lazy me “all” I had to next was to benchmark the lot of them.]]></description><guid isPermaLink="false">6022c644-5499-422b-baab-9124f58a0c69</guid><pubDate>Mon, 31 Dec 2018 13:16:03 +0000</pubDate><dc:creator>Átila Alves Neves</dc:creator></item><item><title>A Rusty Advent of Code</title><link>https://cprimozic.net/blog/a-rusty-aoc/</link><description><![CDATA[For the first time, I took part in the Advent of Code this year. If you haven't heard of it, it's a daily programming challenge that can be solved in any programming language. Rust was very present in the Advent of Code community with people contributing a ton of Rust-related content. In the daily solutions thread on the /r/aoc subreddit, there were always several Rust solutions posted. Advent of Code really helps show off the things that make Rust shine, demonstrating the power and utility of many community-created crates as well as the language itself.]]></description><guid isPermaLink="false">04d1c753-5242-4aa9-bff7-500a717a62bc</guid><pubDate>Thu, 27 Dec 2018 00:00:00 +0000</pubDate><dc:creator>Casey Primozic</dc:creator></item><item><title>Rust &amp; Python—A Gentle Comparison using Simple Neural Networks</title><link>https://blog.digital-horror.com/rust-python-comparison/</link><description><![CDATA[A gentle comparison between Rust & Python from multiple perspectives against a small, relatively simple problem.]]></description><guid isPermaLink="false">6aa36df9-001c-478b-830e-56e2b896303b</guid><pubDate>Sun, 23 Dec 2018 17:10:39 +0000</pubDate><dc:creator>Juxhin Dyrmishi Brigjaj</dc:creator></item><item><title>Converting a Python library to Rust</title><link>https://alantrick.ca/writings/programming/python_to_rust/</link><description><![CDATA[I rewrote a Python project in Rust. The rewrite took a fair bit longer than expected, but the results were good (about 9 times faster and ½ the memory usage). In the process, I learned a fair bit about Rust.]]></description><guid isPermaLink="false">f6f38aae-10b9-4c71-b4fd-c0412a56e064</guid><pubDate>Sat, 22 Dec 2018 00:00:00 +0000</pubDate><dc:creator>Alan Trick</dc:creator></item><item><title>Making Rust Float Parsing Fast and Correct</title><link>https://www.reddit.com/r/rust/comments/a6j5j1/making_rust_float_parsing_fast_and_correct/</link><description><![CDATA[Previously, I wrote about how Rust parsing is atypically slow comparing Rust's libcore implementation to a rudimentary parser I wrote. However, as others noted, the comparison was fairly limited. It didn't compare Rust's implementation to other implementations, such as glibc's strtod or Go's ParseFloat. The parser I implemented wasn't correct, it led to rounding error for most representations, by using floats for intermediate values. Furthermore, the comparisons used data unlikely to be encountered in real-world datasets, overstating the performance differences by forcing Rust to use slower algorithms. So, naturally, I aimed to address all these concerns. And finally, I forgot to disable CPU scaling, meaning CPU throttling could have led to inconsistent benchmarks.]]></description><guid isPermaLink="false">f848772a-4132-4e4f-b817-c4a6dec365b7</guid><pubDate>Sun, 16 Dec 2018 08:39:39 +1100</pubDate><dc:creator>u/ialex32_2</dc:creator></item><item><title>The Swiss Army Knife of Hashmaps</title><link>https://blog.waffles.space/2018/12/07/deep-dive-into-hashbrown/</link><description><![CDATA[A while back, there was a discussion comparing the performance of using the hashbrown crate (based on Google’s SwissTable implementation) in the Rust compiler. In the last RustFest, Amanieu was experimenting on integrating his crate into stdlib, which turned out to have some really promising results. As a result, it’s being planned to move the crate into stdlib.

While the integration is still ongoing, there’s currently no blog post out there explaining SwissTable at the moment. So, I thought I’d dig deeper into the Rust implementation to try and explain how its (almost) identical twin hashbrown::HashMap works.]]></description><guid isPermaLink="false">4ea6d02a-6279-4eef-af2f-bd67424d6de8</guid><pubDate>Fri,  7 Dec 2018 16:58:46 +0000</pubDate><dc:creator>Ravi Shankar</dc:creator></item><item><title>Exploring a shipping puzzle, part 2</title><link>https://kevinlynagh.com/notes/shipping-puzzle/part-2/</link><description><![CDATA[A friend recently told me about a puzzle, which is a great excuse to explore programming craft. My Rust solution was a simple port of my second Clojure solution. The only major difference is that it takes advantage of mutability (which is idiomatic in Rust, unlike in Clojure). The Rust solution runs in about 4.22 ± 0.05 ms, or about 5x faster than the fast Clojure solution.]]></description><guid isPermaLink="false">93922af4-e454-4bfd-a0f2-64c481a6263c</guid><pubDate>Fri,  9 Nov 2018 00:00:00 +0000</pubDate><dc:creator>Kevin Lynagh</dc:creator></item><item><title>How to speed up the Rust compiler in 2018: NLL edition</title><link>https://blog.mozilla.org/nnethercote/2018/11/06/how-to-speed-up-the-rust-compiler-in-2018-nll-edition/</link><description><![CDATA[Niko Matsakis recently blogged about the Rust compiler’s new borrow checker, which implements non-lexical lifetimes (NLL). The new borrow checker is a really nice improvement to Rust, because it accepts many sound programs that the old borrow checker rejected.]]></description><guid isPermaLink="false">14df2f8d-908b-4836-966f-b307f763319d</guid><pubDate>Tue,  6 Nov 2018 00:09:41 +0000</pubDate><dc:creator>Nicholas Nethercote</dc:creator></item><item><title>Parsing logs 230x faster with Rust</title><link>https://andre.arko.net/2018/10/25/parsing-logs-230x-faster-with-rust/</link><description><![CDATA[Perhaps surprisingly, one of the most challenging things about operating RubyGems.org is the logs. A single day of request logs is usually around 500 gigabytes on disk. So every day, we generate about 500 files that are 85MB on disk, and contain about a million streaming JSON objects that take up 1GB when uncompressed. What we want out of those files is incredibly tiny—a few thousand integers, labelled with names and version numbers. Without any real idea of how to get those counts out of S3, I started by writing a proof of concept Ruby script that could parse one of the 500 log files and print out stats from it. Even on my super-fast laptop, my prototype script would take more than 16 hours to parse 24 hours worth of logs.]]></description><guid isPermaLink="false">80d71718-3acf-49d1-ad47-3e05c5da17a2</guid><pubDate>Thu, 25 Oct 2018 00:00:00 +0000</pubDate><dc:creator>Andre Arko</dc:creator></item><item><title>Rust RwLock and Mutex Performance Oddities</title><link>https://fy.blackhats.net.au/blog/html/2018/10/19/rust_rwlock_and_mutex_performance_oddities.html</link><description><![CDATA[Recently I have been working on Rust datastructures once again. In the process I wanted to test how my work performed compared to a standard library RwLock and Mutex. On my home laptop the RwLock was 5 times faster, the Mutex 2 times faster than my work.

So checking out my code on my workplace workstation and running my bench marks I noticed the Mutex was the same - 2 times faster. However, the RwLock was 4000 times slower.]]></description><guid isPermaLink="false">8157d765-72cb-40a0-b59f-6b4ae2ae173d</guid><pubDate>Fri, 19 Oct 2018 00:00:00 +0000</pubDate><dc:creator>Firstyear</dc:creator></item><item><title>Oxidizing Python: Speeding up URL quoting by 10× using Rust</title><link>https://tech.blue-yonder.com/oxidizing-python-speeding-up-urlquoting-by-using-rust/</link><description><![CDATA[Recently a colleague of mine told me about a small bottleneck with url quoting since we are quoting a lot of storage keys at least once when loading or storing a dataset. To speed it up, we are going to write a C-Library in Rust and invoke it from Python.]]></description><guid isPermaLink="false">0d41cc9f-584c-4da3-8516-a567cc923279</guid><pubDate>Mon,  8 Oct 2018 10:48:12 +0000</pubDate><dc:creator>Markus Klein</dc:creator></item><item><title>A Case Study in Heaptrack</title><link>https://speice.io/2018/10/case-study-optimization.html</link><description><![CDATA[When I first started building the dtparse crate, my intention was to mirror as closely as possible the equivalent Python library. Python, as you may know, is garbage collected. Very rarely is memory usage considered in Python, and I likewise wasn’t paying too much attention when dtparse was first being built.

This lackadaisical approach to memory works well enough, and I’m not planning on making dtparse hyper-efficient. But every so often, I’ve wondered: “what exactly is going on in memory?”]]></description><guid isPermaLink="false">917520f5-6f74-4e91-87e1-40d2360c0e46</guid><pubDate>Mon,  8 Oct 2018 00:00:00 -0400</pubDate><dc:creator>Bradlee Speice</dc:creator></item><item><title>No, pest is not faster than nom</title><link>https://unhandledexpression.com/general/2018/10/04/no-pest-is-not-faster-than-nom.html</link><description><![CDATA[But today (October 4th, 2018), the pest website featured a very misleading graph. Yes, a pest 2.0 parser that does not convert the input to Rust types is indeed faster than a nom 4.0 parser that does convert the input to Rust types. But what happens if I write a nom 4.0 parser that does not convert its input to Rust types?]]></description><guid isPermaLink="false">dceefb77-f60e-4864-80e4-1dfb50c52cce</guid><pubDate>Thu,  4 Oct 2018 00:00:00 +0000</pubDate><dc:creator>Geoffroy Couprie</dc:creator></item><item><title>Going Four Times Faster using Multi-Threading</title><link>http://worthe-it.co.za/programming/2018/10/03/going-four-times-faster-with-multithreading.html</link><description><![CDATA[Rust makes writing parallel code safe. Rayon makes it easy.]]></description><guid isPermaLink="false">55b31f9c-fa63-4890-bcf3-5f9b899c197f</guid><pubDate>Wed,  3 Oct 2018 00:00:00 +0000</pubDate><dc:creator>Justin Worthe</dc:creator></item><item><title>The relative performance of C and Rust</title><link>http://dtrace.org/blogs/bmc/2018/09/28/the-relative-performance-of-c-and-rust/</link><description><![CDATA[I reimplemented a body of C software in Rust, and it performed better for the same task; what’s going on? And is there anything broader we can say about these results?

To explore this, I ran some statemap rendering tests on SmartOS on a single-socket Haswell server (Xeon E3-1270 v3) running at 3.50GHz. The C version was compiled with GCC 7.3.0 with -O2 level optimizations; the Rust version was compiled with 1.29.0 with --release. All of the tests were run bound to a processor set containing a single core; all were bound to one logical CPU within that core, with the other logical CPU forced to be idle. cpustat was used to gather CPU performance counter data, with one number denoting one run with pic0 programmed to that CPU performance counter. The input file (~30MB compressed) contains 3.5M state changes, and in the default config will generate a ~6MB SVG. ]]></description><guid isPermaLink="false">23617503-ac10-4027-bd53-a98ad82c7d1e</guid><pubDate>Sat, 29 Sep 2018 01:28:17 +0000</pubDate><dc:creator>Bryan Cantrill</dc:creator></item><item><title>lolbench: automagically and empirically discovering Rust performance regressions</title><link>https://blog.anp.lol/rust/2018/09/29/lolbench/</link><description><![CDATA[lolbench compiles ~350 benchmarks with every Rust nightly. It then runs them and highlights potential performance regressions in the standard library and the output of the compiler. Each toolchain’s run is summarized with a list of likely candidates, as seen in the image below, and we’re now getting started using these to safeguard the performance of Rust programs. Come help!]]></description><guid isPermaLink="false">307ed024-4949-4c53-80cf-c1e60a57e0a7</guid><pubDate>Sat, 29 Sep 2018 00:00:00 +0000</pubDate><dc:creator>Adam Perry</dc:creator></item><item><title>The evolution of performance in ppbert</title><link>https://vfoley.xyz/ppbert-perf-evolution/</link><description><![CDATA[Today I released ppbert 0.8.4. This release also marks the first time that one of my original test files can be pretty printed in less than a second. I’ll use this occasion to look back on ppbert and how I was able to improve its performance, little by little.]]></description><guid isPermaLink="false">90573eab-4e15-48a4-9c53-8f3466356845</guid><pubDate>Wed, 19 Sep 2018 00:00:00 +0000</pubDate><dc:creator>Vincent Foley</dc:creator></item><item><title>Alacritty now supports scrollback</title><link>https://jwilm.io/blog/alacritty-lands-scrollback/</link><description><![CDATA[Alacritty, the OpenGL terminal emulator written in Rust, now supports scrollback! Performance has improved, and we've got benchmarks to share.]]></description><guid isPermaLink="false">4f00ccb1-9966-4c6d-83e9-0e8032ff68a4</guid><pubDate>Sun, 16 Sep 2018 17:00:00 -0700</pubDate><dc:creator>Joe Wilm</dc:creator></item><item><title>A Sudoku Solver &amp; Generator 🔢</title><link>https://blog.ryanlevick.com/posts/sudoku-solver-generator/</link><description><![CDATA[For a small side project I’m working on, I’m using a Sudoku puzzle solver and puzzle generator that I’ve written in Rust. The experience was fun, so I thought I’d write up a little bit about the algorithm I’ve used and some interesting stats about how it performs.]]></description><guid isPermaLink="false">e3bb7297-9e70-4070-a2aa-8bb8f9a5eaf2</guid><pubDate>Sun, 16 Sep 2018 00:00:00 +0000</pubDate><dc:creator>Ryan Levick</dc:creator></item><item><title>Measuring SmallVec Footprint with Smallvectune</title><link>https://llogiq.github.io/2018/09/13/smallvec.html</link><description><![CDATA[Rust is all about paying only for what you use, and gives us plenty tools to eliminate unneeded allocation. One of the tools that is used in a lot of crates (crates.io shows 98 dependent crates) is SmallVec. It is also used in the Rust compiler. I recently got around to speed up the operation of getting a SmallVec from a slice of copyable data. In short, they’re awesome.]]></description><guid isPermaLink="false">48eac964-8486-4cd6-83cb-3bca4ff5b616</guid><pubDate>Thu, 13 Sep 2018 00:00:00 +0000</pubDate><dc:creator>Llogiq</dc:creator></item><item><title>Parallelizing PNG, part 7: Rust profiling on Linux</title><link>https://brionv.com/log/2018/09/11/parallelizing-png-part-7-rust-profiling-on-linux/</link><description><![CDATA[I already covered some inner-loop optimization tricks for low-level Rust code in mtpng, but how do you check how fast bits of your code are anyway?

The way to go is to use a sampling-based profiler native to your operating system. I’ve done most of my detailed profiling on Linux, using the “perf” tool.]]></description><guid isPermaLink="false">2cfb43bc-99fe-4fea-beb0-8e422c55376a</guid><pubDate>Tue, 11 Sep 2018 19:52:29 +0000</pubDate><dc:creator>Brion Vibber</dc:creator></item><item><title>Parallelizing PNG: Choosing Rust for mtpng</title><link>https://brionv.com/log/2018/09/09/parallelizing-png-part-5-choosing-rust-for-mtpng/</link><description><![CDATA[In my last post I wrapped up the patches to improve perceived performance of screenshots on the Linux GNOME desktop. With that done, why not implement my crazy plan for parallel PNG encoding to speed the actual save time?]]></description><guid isPermaLink="false">d8bbfe78-53bb-49df-9d7b-092d306492c0</guid><pubDate>Sun,  9 Sep 2018 23:32:41 +0000</pubDate><dc:creator>Brion Vibber</dc:creator></item><item><title>Rust Faster SIMD edition</title><link>https://llogiq.github.io/2018/09/06/fast.html</link><description><![CDATA[It’s been a while since I’ve been playing the benchmarksgame with Rust. But I recently found an interesting crate called packed_simd which had a SIMD-ified version of some benchmarks, so as Rust stable now has stdsimd, we should be able to speed up our benchmarks quite a bit.]]></description><guid isPermaLink="false">bf9e8fe4-c788-47c9-9a8a-27ee3fa0b20f</guid><pubDate>Thu,  6 Sep 2018 00:00:00 +0000</pubDate><dc:creator>Llogiq</dc:creator></item><item><title>Time difference between L1 cache fetch and memory fetch</title><link>https://nitish.ch/notes/time-difference-between-l1-cache-fetch-and-memory-fetch/</link><description><![CDATA[Aim: Measure how fast a fetch from L1 cache is when compared to a fetch from memory. Instead of writing pure assembly code, we will use Rust's inline assembly feature.]]></description><guid isPermaLink="false">3c7f3466-8495-4684-b0c2-e0b26ca9234c</guid><pubDate>Sun,  2 Sep 2018 22:15:24 +0000</pubDate><dc:creator>Nitish Chinta</dc:creator></item><item><title>Benchmarking a Rust web application</title><link>https://klausi.github.io/rustnish/2018/08/31/benchmarking-a-rust-web-application.html</link><description><![CDATA[I set out out my goal 9 for Rustnish: Write benchmark code that compares runtime performance of Rustnish against Varnish. Use cargo bench to execute the benchmarks.

The basic idea of a performance test here is to send many HTTP requests to the web service (the reverse proxy in this case) and measure how fast the responses arrive back. Comparing the results from Rustnish and Varnish should give us an idea if our performance expectations are holding up.]]></description><guid isPermaLink="false">715ee4bf-9164-4c58-aa4f-93f088859281</guid><pubDate>Fri, 31 Aug 2018 00:00:00 +0000</pubDate><dc:creator>klausi</dc:creator></item><item><title>Comparing code counters</title><link>https://github.com/Aaronepower/tokei/blob/master/COMPARISON.md</link><description><![CDATA[This document is a compilation of various benchmarks and comparisons between code counters, namely tokei, cloc, scc, and loc. This document seeks to compare performance, and accuracy of the code counters. polyglot is not currently included as it was unabled to be installed on the machine at the time of writing.]]></description><guid isPermaLink="false">f9bf5234-b8f7-4a0d-9908-e1cf35c6ba8b</guid><pubDate>Sat, 25 Aug 2018 21:00:24 +0000</pubDate><dc:creator>Aaron Power</dc:creator></item><item><title>Rust, meet q</title><link>https://blog.redsift.com/labs/rust-meet-q/</link><description><![CDATA[We leverage the elegance of kdb+ and the power of Rust to create data applications that can process data at the rate of tens of GB/second on consumer grade hardware.]]></description><guid isPermaLink="false">ba6b3643-c414-4efe-90a4-4ea6fd45ef3d</guid><pubDate>Thu, 23 Aug 2018 13:08:16 +0000</pubDate><dc:creator>Rahul</dc:creator></item><item><title>Reading files quickly in Rust</title><link>https://boyter.org/posts/reading-files-quickly-in-rust/</link><description><![CDATA[With the latest release of 1.27 of Rust (SIMD support) the code counters written in Rust were suddenly a lot faster in Linux. In fact it meant that the fastest one tokei was suddenly faster than my scc for almost all tests. In addition a new project polyglot written in a language I have never heard of ATS popped up which is also now faster than my Go program for any repository when running on a machine with less than 8 cores.]]></description><guid isPermaLink="false">d26e4a2d-a00d-4ab2-95e3-326ded5d1fe0</guid><pubDate>Mon, 20 Aug 2018 00:00:00 +0000</pubDate><dc:creator>Ben Boyter</dc:creator></item><item><title>Learning SIMD with Rust by finding planets</title><link>https://medium.com/@Razican/learning-simd-with-rust-by-finding-planets-b85ccfb724c3</link><description><![CDATA[Rust 1.27.0 has brought SIMD (Single Instruction Multiple Data), also known as vectorization, to stable Rust. If you read the announcement, you will see that SIMD should bring performance enhancements to our applications if we learn how to use it properly. But, for that let's first dive into how SIMD works.]]></description><guid isPermaLink="false">98469a08-b569-403f-a3ed-6049ff91c0b1</guid><pubDate>Mon,  2 Jul 2018 20:47:57 +0000</pubDate><dc:creator>Iban Eguia</dc:creator></item><item><title>Optimising path tracing: the last 10%</title><link>https://bitshifter.github.io/blog/2018/06/20/the-last-10-percent/</link><description><![CDATA[In my last post on optimising my Rust path tracer with SIMD I had got withing 10% of my performance target, that is Aras’s C++ SSE4.1 path tracer. From profiling I had determined that the main differences were MSVC using SSE versions of sinf and cosf and differences between Rayon and enkiTS thread pools. The first thing I tried was implement an SSE2 version of sin_cos based off of Julien Pommier’s code that I found via a bit of googling. This was enough to get my SSE4.1 implementation to match the performance of Aras’s SSE4.1 code. I had a slight advantage in that I just call sin_cos as a single function versus separate sin and cos functions, but meh, I’m calling my performance target reached.

The other part of this post is about Rust’s runtime and compile time CPU feature detection and some wrong turns I took along the way.]]></description><guid isPermaLink="false">495d9d83-2572-43cb-8f9d-c5c404ab83fe</guid><pubDate>Wed, 20 Jun 2018 00:00:00 +0000</pubDate><dc:creator>bitshifter</dc:creator></item><item><title>How to speed up the Rust compiler some more in 2018</title><link>https://blog.mozilla.org/nnethercote/2018/06/05/how-to-speed-up-the-rust-compiler-some-more-in-2018/</link><description><![CDATA[Since my last post, rustc-perf — the benchmark suite, harness and visualizer — has seen some improvements. First, some new benchmarks were added: cargo, ripgrep, sentry-cli, and webrender. Also, the parser benchmark has been removed because it was a toy program and thus not a good benchmark.]]></description><guid isPermaLink="false">9a7909ce-8831-4f06-b32e-55a4a3df9116</guid><pubDate>Tue,  5 Jun 2018 00:05:01 +0000</pubDate><dc:creator>Nicholas Nethercote</dc:creator></item><item><title>Optimising path tracing with SIMD</title><link>https://bitshifter.github.io/blog/2018/06/04/simd-path-tracing/</link><description><![CDATA[Following on from path tracing in parallel with Rayon I had a lot of other optimisations I wanted to try. In particular I want to see if I could match the CPU performance of @aras_p’s C++ path tracer in Rust. He’d done a fair amount of optimising so it seemed like a good target to aim for. To get a better comparison I copied his scene and also added his light sampling approach which he talks about here. I also implemented a live render loop mimicking his.]]></description><guid isPermaLink="false">019cfc86-4560-40cc-8619-30d04e30699b</guid><pubDate>Mon,  4 Jun 2018 00:00:00 +0000</pubDate><dc:creator>bitshifter</dc:creator></item><item><title>RustFest Paris Workshop: Fastware</title><link>http://troubles.md/posts/rustfest-2018-workshop/</link><description><![CDATA[It’s often said1 that the slowest code is that which has been optimised without benchmarks. You wouldn’t expect your code to work if you never ran it, so why should you expect it to be fast if you never benchmarked it? Writing good benchmarks is a bit of an art, because it’s really easy to accidentally write benchmarks that make your code seem fast, when really the compiler is applying some optimisations that work in the side-effect-free world of the benchmark but can no longer get applied when you put it out into the wild.]]></description><guid isPermaLink="false">cb90dbcf-0621-4e96-83ca-62deac7e8a3d</guid><pubDate>Tue, 22 May 2018 11:22:48 +0200</pubDate><dc:creator>troubles.md</dc:creator></item><item><title>Porting Rust Benchmarks To Criterion</title><link>https://llogiq.github.io/2018/05/18/criterion.html</link><description><![CDATA[A few weeks ago, I set out to convert bytecount’s benchmarks to criterion, a statistics-driven benchmarking framework started by Jorge Aparicio and maintained by Brook Heisler.

Before, bytecount used bencher for its benchmarks, which is a straight port of the unstable, nightly-only std::test benchmark framework, extended to work with stable Rust. This was a great benefit compared to std::test, because now we could benchmark on all Rust versions (stable, beta, nightly, some specific version) without needing to fear regressions.]]></description><guid isPermaLink="false">51352c9c-dac9-4e94-a90f-5e953057a01a</guid><pubDate>Fri, 18 May 2018 00:00:00 +0000</pubDate><dc:creator>Llogiq</dc:creator></item><item><title>Improving SmallVec&apos;s speed by 60% and why that shouldn&apos;t matter to you</title><link>http://troubles.md/posts/improving-smallvec/</link><description><![CDATA[smallvec is a library by the Servo team for reducing the number of allocations for dynamic arrays in the case that most of those arrays are below a certain size. Because malloc is fast, for many cases it’s actually slower to use SmallVec than just using Vec because the one-time cost of the initial allocation is dwarfed by the lifetime cost of SmallVec’s increased complexity. You can see that switching to Vec actually improves speed on many of SmallVec’s own benchmarks.]]></description><guid isPermaLink="false">da6bf82a-304e-42bd-9209-fbb8ee975fcd</guid><pubDate>Thu, 17 May 2018 14:44:51 +0200</pubDate><dc:creator>troubles.md</dc:creator></item><item><title>The Rust compiler is getting faster</title><link>https://blog.mozilla.org/nnethercote/2018/05/17/the-rust-compiler-is-getting-faster/</link><description><![CDATA[TL;DR: The Rust compiler has gotten 1.06x–4x faster over the past month.]]></description><guid isPermaLink="false">a9218983-121e-4d4f-a02f-fd30cb6cf9b2</guid><pubDate>Thu, 17 May 2018 04:08:48 +0000</pubDate><dc:creator>Nicholas Nethercote</dc:creator></item><item><title>Dropping drops</title><link>https://barrielle.cedeela.fr/research_page/dropping-drops.html</link><description><![CDATA[Recently, a benchmark made it to the top of /r/programming, featuring Rust among other languages, and I was a bit surprised to see that the idiomatic Rust program was not competitive with the best-tuned C++ solution. The benchmark implements a binary tree, and the C++ solution leverages raw pointers while Rust would use an Option<Box<Node>> to represent its tree. Since Option knows that Box is non-nullable, it should compile down to a raw pointer. Quickly inspecting the Rust and C++ versions would not let me find where the performance difference came from.]]></description><guid isPermaLink="false">f676f9eb-ea74-4dc5-a393-1f23a7886678</guid><pubDate>Thu, 17 May 2018 00:00:00 +0200</pubDate><dc:creator>Vincent Barrielle</dc:creator></item><item><title>Performance experiments with matrix multiplication</title><link>https://vorner.github.io/2018/05/12/Mat-perf.html</link><description><![CDATA[One of Rust’s design goals is to be fast. That actually needs two distinct things from the language. First, is it shouldn’t introduce too much (preferably zero) overhead for its abstractions and be fast out of the box. Many people coming from the high level languages (python, javascript, …) find this to be the case ‒ just type the program, compile it (with --release) and it’s reasonable fast. The other, no less important, is allowing the programmer to tweak some knobs when trying to squeeze a bit more speed out of the program.

I’ve decided to test the second a bit and see how far I could go. I’ve chosen matrix multiplication as a case study, for several reasons. I’ve played with it before (in my master’s thesis), it’s relatively simple and the effects of optimizing it can be great. For simplicity, I’ve decided to multiply only square matrices with power-of-two sizes, but these restrictions can be lifted in a real implementation without significantly loosing performance ‒ only the code gets somewhat more complex and hairy.]]></description><guid isPermaLink="false">d18e865c-71c2-4c7e-85ff-7d84e1999055</guid><pubDate>Sat, 12 May 2018 00:00:00 +0000</pubDate><dc:creator>Michal &apos;vorner&apos; Vaner</dc:creator></item><item><title>How a Rust upgrade more than tripled the speed of my code</title><link>http://troubles.md/posts/the-power-of-compilers/</link><description><![CDATA[I’d like to share a quick story about the sheer power of LLVM and the benefits of using higher-level languages over assembly.]]></description><guid isPermaLink="false">97a47f42-b32a-45df-8473-892b94367048</guid><pubDate>Fri, 11 May 2018 16:07:31 +0200</pubDate><dc:creator>troubles.md</dc:creator></item></channel></rss>